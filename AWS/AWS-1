										AWS



Types of cloud computing

Saas: Software as a service
Paas: Platform as a service
Iaas: Infrastructure as a service

AZs : availibility zones , each region has 2 AZs. AZs are requested by a region code. followed by a letter identified ed: us-east-1-a

								
########################################################################################################################################################################################################

									IAM 

AWS Identity and Access Management (IAM) is a web service that enables Amazon Web Services
(AWS) customers to manage users and user permissions in AWS. The service is targeted at
organizations with multiple users or systems in the cloud that use AWS products such as Amazon
EC2, Amazon SimpleDB, and the AWS Management Console. With IAM, you can centrally
manage users, security credentials such as access keys, and permissions that control which AWS
resources users can access.

Creating IAM user: 
Login to AWS account => Services => IAM

Select users => Add user => username

Select one of the options either Programmatic access (Key-based access for AWS-CLI) or AWS Management Console access.

If you select AWS Management Console access, you need to select Autogenerated password or custom password.

Click on Next Permissions

Attach Policy to the User.

By default, IAM users don't have permission to create or modify Amazon EC2 resources, or
perform tasks using the Amazon EC2 API. (This means that they also can't do so using the Amazon
EC2 console or CLI).When you attach a policy to a user or group of users, it allows or denies the users permission to
perform the specified tasks on the specified resources. So while creating user we have to assign the
policies which has to be performed by particular user. There are some predefined policies in IAM,
we can attach the existing policies or create new policy. Here in this example we are attaching
Administrator Access in which the user gets all administrator permissions

After creating user we get a Download.csv file, which contains the credentials of
the user and url to login to the AWS console.

Then there is option if MFA if you want to do it. (Multi factor authentication)
Setting up MFA:
AWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of
protection on top of your user name and password. With MFA enabled, when a user signs in to an
AWS w
ite, they will be prompted for their user name and password (the first factor—what they
know), as well as for an authentication code from their AWS MFA device (the second factor—what
they have). Taken together, these multiple factors provide increased security for your AWS account
settings and resources.
You can enable MFA for your AWS account and for individual IAM users you have created under
your account. MFA can be also be used to control access to AWS service APIs.

Click on Username

Click on Assigned MFA device
Steps:
=> Download Google Authenticator in your Smartphone
=> Open Google Authenticator
=> Click plus symbol
=> Scan the barcode
=> Enter Auth code1
=> Enter second auth code.
Use the highlighted URL for your user to login to AWS account with IAM user.

##################################################################################################################################################################################################



AMI (Amazon machine image)

Copy your instance, can create whether instance is running or stopped.

An Amazon Machine Image (AMI) is a special type of virtual appliance that is used to
create a virtual machine within the Amazon Elastic Compute Cloud ("EC2"). It serves as the basic
unit of deployment for services delivered using EC2.

AMI Creation
Create AMI of the instance which we will use to spin web02 instance.
Web02 instance is exactly similar to web01, so instead to creating new instance from scratch and
setting up apache, we can create an AMI (image) of web01 instance and can spin as many as web
instances we want.
Select the instance in which we have to create an image. Click on actions, select image and click on
create image.You will get a create image dialog box as shown below. Give proper name and
description for image and click on create image.



AMI provides the information require to launch an instance.

You can turn your EC2 instances into AMIs so you can create copies of your servers.
An AMI holds the following information:
- A template foe the root  volume for the instance (EBS snapshot or Instance store template) eg. an operating system, an application server, and applications.
- launch permissions that control which AWS accounts can use the AMi to launch instances.
- A block device mapping that specifies the volume to attach to the instances when its launched.
- AMIs are region specific.

##################################################################################################################################################################################################


AWS Auto Scaling

Auto Scaling is a web service designed to launch or terminate Amazon EC2 instances automatically
based on user-defined policies, schedules, and health checks. You can use Auto Scaling to help
ensure that you are running your desired number of Amazon EC2 instances. Auto Scaling can also
automatically increase the number of Amazon EC2 instances during demand spikes to maintain
performance and decrease capacity during lulls to reduce costs. Auto Scaling is well suited both to
applications that have stable demand patterns or that experience hourly, daily, or weekly variability
in usage.
Benefits of Auto Scaling:
Adding Auto Scaling to your application architecture is one way to maximize the benefits of the
AWS cloud. When you use Auto Scaling, your applications gain the following benefits:
➢ Better fault tolerance. Auto Scaling can detect when an instance is unhealthy, terminate it,
and launch an instance to replace it. You can also configure Auto Scaling to use multiple
Availability Zones. If one Availability Zone becomes unavailable, Auto Scaling can launch
instances in another one to compensate.
➢ Better availability. Auto Scaling can help you ensure that your application always has the
right amount of capacity to handle the current traffic demands.
➢ Better cost management. Auto Scaling can dynamically increase and decrease capacity as
needed. Because you pay for the EC2 instances you use, you save money by launching
instances when they are actually needed and terminating them when they aren't needed.



Setup for Auto Scaling: Here is process for setting up the basic infrastructure for Auto Scaling.
For practice of auto scaling we need EC2 instances with following details.
• Create an EC2 instance of CentOS6 with instance type t2.micro,Security Group – SSH =>
MyIP, HTTP => Anywhere.
• Download the Private Keypair for Access.
• Install HTTPD service and start it.


Now once we have our instance ready we will create an AMI for the purpose of autoscaling.
Creating AMI:
Select an instance, click on Actions and select Image Create Image

Creating AWS ELB:
Once we have our AMI ready we can start creating ELB, please refer to below screenshots.
Select Load Balancer on the left pane of the EC2 Dashboard.

Once we have our ELB ready, wait for the instance in ELB to become healthy and use ELB DNS
name to verify if the Worthy website is accessible.
❖ Create Launch Configuration:
First we need to create and configure a Launch Configuration. From the EC2 Management
Dashboard, select the AutoScaling Groups option from the navigation pane as shown in the
following screenshot. This will bring up the Auto Scaling Groups dashboard. Next, select the Create
Auto Scaling group to bring up the Auto Scaling setup wizard.

Click on Create Auto Scaling group.
❖ Choose the AMI that we created previously.
❖ Choose an instance type: Select t2.micro for free tier.
❖ Configure details: Provide a Name for the Launch Configuration. Select the Enable
CloudWatch detailed monitoring checkbox if you wish to have your instances monitored
for a duration of 60 seconds. By default your instances will be monitored by
CloudWatch for a minimum period of 300 seconds (five minutes) for no charge at all.
Selecting detailed monitoring will incur additional charges, so use it with caution.

Note: Enabling the CloudWatch detailed monitoring option is highly recommended in case the
instances belong to a production environment.
Once the configure details are filled out, you can even set the instance’s IP addressing scheme by
selecting the Assign a public IP address to every instance option from the Advanced Details section.
❖ Add storage: You can add an optional Volume to your instances by selecting the Add
New Volume button on the Add Storage page. Here I have not provided any additional
volumes to my instances. Click on Next: Configure Security Group to either create or
select an existing security group for your auto scaled instances.

❖ Configure Security Group: From the Configure Security Group page, select an
appropriate Security Group for your Auto Scaled instances. Since we are working with web
server instances we use the following inbound rules:
SSH MyIP and HTTP Anywhere.

❖ Review: Make sure that all the details are configured correctly. Once verified, click on
Create Launch Configuration to complete the process.

Creating the Auto Scaling Group: An Auto Scaling group is a collection of EC2 instances, and
the core of the Auto Scaling service. You create an Auto Scaling group by specifying the launch
configuration you want to use for launching the instances and the number of instances your group
must maintain at all times. You also specify the Availability Zone in which you want the instances
to be launched.
Configure Auto Scaling group details:
The first step in creating Auto Scaling Group requires providing a suitable name for Auto Scaling
Group as well as its Network and Load Balancing details.
Fill in the required fields as per your requirements:
• Group name: Provide a suitable name for your Auto Scaling Group.
• Group size: Here, enter the desired capacity for your Auto Scaling Group.
The value entered here represents the number of instances. Here I have chosen 2
instances just for practice.
• Network: If you are launching a t2.micro instance, you must select a VPC in
Network. Otherwise, if your account supports EC2-Classic and you are launching a
type of instance that doesn't require a VPC, you can select either Launch into EC2-
Classic or a VPC.
• Subnet: If you select a VPC in the previous step, select one or more subnets from
Subnet. If you select EC2-Classic in the previous step, select one or more
Availability Zones from Availability Zones. In my case, I have selected two subnets,
each created in a different AZ.


Note: Each instance in this Auto Scaling Group will be provided with a public IP address.
After filling the basic details we will configure the Advanced Details section of our Auto Scaling
Group:
Load Balancing: Since we have already created and configured our ELB, we will be using that to
balance out incoming traffic for our instances.
Health Check Type: We can use either our EC2 instances or ELB as a health check mechanism to
make sure that your instances are in a healthy state. By default, Auto Scaling will check your EC2
instances periodically for their health status. If an unhealthy instance is found, Auto Scaling will
immediately replace that with a healthy one. Here, I have selected ELB as my health check type, so
all the instances health checks are now performed by the ELB itself.
Health Check Grace Period: By default, this value is set to 300 seconds.
Configure Scaling policies:
You can create a scaling policy that uses CloudWatch alarms to determine when your Auto Scaling
group should scale out or scale in. Each CloudWatch alarm watches a single metric and sends
messages to Auto Scaling when the metric breaches a threshold that you specify in your policy. You
can use alarms to monitor any of the metrics that the services in AWS that you're using send to
CloudWatch, or you can create and monitor your own custom metrics.

There are two policies used by an Auto Scaling Group: one to increase the instance count based on
certain alarms and the other to decrease the instance count.
Increase Group Size policy, as shown in the following screenshot:
Name: Provide a suitable name for your scale-out policy.
Execute policy when: Here you have to select a pre-configured alarm using
which the policy will get triggered. As we are configuring this for the first time, select the Add new
alarm option. This will pop up the Create Alarm dialog, as shown in the
following screenshot:
Scaling as per CPU utilization: We want our Auto Scaling Group to be monitored based on the
CPU Utilization metric for an interval of 5 minutes. If the average CPU Utilization is greater than or
equal to 50 percent for at least one consecutive period, then send a notification mail.

Set the action after trigger:
With the basic alarm now set, you can configure your policy what action it has to take if the
particular threshold is breached. Select Add from the dropdown list and provide a suitable number
of instances that you wish to add when a certain condition matches. Here I have created a four-step
scaling policy that first adds one instance to the group when the average CPU utilization is within a
particular threshold range, such as 70-80 percent. Next, another two instances are added when the
CPU utilization increases to 80-90 percent.

Set scale in alarm trigger: Similarly set the threshold for decreasing group size and terminating
the instances.

When the decreasing threshold is reached another instance is removed when the CPU utilization
decreases to less than 60.

We can tag our instances for organizing, managing and identifying our instances more effectively
and efficiently. Next click on review which gives all the details you have selected for auto scaling
group and click on create auto scaling group.

Then you can see a message as Successfully created auto scaling group.

########################################################################################################################################################################################################


			Amazon S3 (Simple Storage Service)


Filesystem: Managaes data as a file/file hiearchy.
Block storage: Manages data as block within sectors and tracks.
s3 object: Objects contain your data.They are like files.
s3 is a universal namespace so bucket names must be unique(like a domain name)
Standard (default) Replicates across at least three AZs.

s3 Cross region replication (CRR)
when enabled any object that is uploaded will be automatically replictaed to another region, provides higher durability and potential disaster ecovery for objects. You must have versioning turned on on both the source and destinatiion buckets. you can have CRR replicate to another AWs account.

Versioning : 
- stores all version of an object in s3.
- Once enabled cant be deleted,only can be suspended on the bucket.

s3 presigned URLs:

generates a URl which provides you temporary access to an object to either upload/download object data. presigned URls are commonly used to provide access to private objects. you can use AWS CLO or AWS SDK to generate presigned URls.
# aws s3 presigned s3://mybucket/myobject --expires-in 300

s3 MFA delete:

It makes sure you cant delete any object till you provide the MFA code..
- Can be enabled only when, Aws cli used to turn on the MFA
- The bucket must have versioning turned on.

#aws s3api bucket-versioning --bucket bucketname --versioning-configuration status=Enabled,MFADelete=Enabled --mfa "mfa serial no. mfs-code"

Only the bucket owner logged in as Root can delete the object from the bucket.

Versioning: Gives you an option to save your file while enabling versioning even if you delete the file you can still get ti through version(show).
- after enabling version you will get the version Id else it will be null.


Amazon Simple Storage Service (Amazon S3) is storage for the Internet. You can use Amazon S3 to store and
retrieve any amount of data at any time, from anywhere on the web. You can accomplish these tasks using the
simple and intuitive web interface of the AWS Management Console.
Following are some of the advantages of Amazon S3 Service:
• Create Buckets – Create and name a bucket that stores data. Buckets are the fundamental
container in Amazon S3 for data storage.
• Store data in Buckets – Store an infinite amount of data in a bucket. Upload as many
objects as you like into an Amazon S3 bucket. Each object can contain up to 5 TB of data.
Each object is stored and retrieved using a unique developer-assigned key.
• Download data – Download your data or enable others to do so. Download your data any
time you like or allow others to do the same.
• Permissions – Grant or deny access to others who want to upload or download data into
your Amazon S3 bucket. Grant upload and download permissions to three types of users.
Authentication mechanisms can help keep data secure from unauthorized access.
• Standard interfaces – Use standards-based REST and SOAP interfaces designed to work
with any Internet-development toolkit.

Amazon S3 stores data as objects within buckets. An object consists of a file and optionally any metadata
that describes that file.
To store an object in Amazon S3, you upload the file you want to store to a bucket. When you upload a file,
you can set permissions on the object as well as any metadata.
Buckets are the containers for objects. You can have one or more buckets. For each bucket, you can control
access to it (who can create, delete, and list objects in the bucket), view access logs for it and its objects, and
choose the geographical region where Amazon S3 will store the bucket and its contents.
Creating Amazon S3 Bucket:
Go to Amazon S3 Service page from main Dashboard, Click on Create Bucket.

You will see create bucket dialog box in which you have to give Bucket Name and Region. Click
on Create.

Bucket is created and now you can upload files into that which you want to store. Click on Upload
and choose Add files to choose the file to be upload.

Choose a file to upload, and then choose Open.

The file is uploaded, now you have to give permissions to the file, who has to open/download, view
and edit permissions etc. So select the file right click on it you can see properties where you can
change/give permissions.

In the permissions tab you can add permissions as required. Here I have given permissions to
everyone. Once done click on save. Now you can see the details of the bucket, it gives the url for
the file which you can open it through browser.

Similarly, we can create folders in bucket, move the objects and delete buckets to prevent further
charges if you no longer need to store the objects that you have uploaded.


aws cli commands

# aws s3 ls
# aws s3 ls s3://himanshu (show contents of bucket)
# aws s3 mb s3://himanshu-new
# aws s3 rb bucketname
# aws s3 rm s3://himanshu-new --recursive (all data od=f bucket)
# aws s3 cp ~/Desktop/home s3://himanshu-new/home/xx (Upload to bucket)
# aws s3 cp s3://himanshu-new/home/xx ~/Desktop/xx (Download from Bucket)
# aws s3 presing s3://himanshu/document --expires-in 300 

We cant delete the bucket till versioning is there coz there may be previous files. So in this case we have to delete all the versions first then only we can delete the bucket.
#######################################################################################################################################################################################################



				AWS CLI [Amazon Command Line Interface]

Installing & Configuration AWS-CLI
❖ Create IAM User from AWS Console
AWS Identity and Access Management (IAM) is a web service that helps you securely control
access to AWS resources for your users.
1. Open IAM dashboard
2. Creat user with programmatic access.
3. Select Attach Existing Policies Directly
Check Policy Name - AdminstrativeAccess & Click Next
Click Create user

After creating the user, Download Credentials file for Access Key and Security Key for AWS
configuration for CLI

#aws configure

The aws configure set command can be used to set a single configuration value in the AWS config
file.
Default region is the name of the region you want to make calls against by default. This is usually
the region closest to you, but it can be any region.

Configuration and Credential Files
We can add additional configure profile by adding entries to config and credential files.
The CLI stores credentials specified with aws configure in a local file named credentials in a folder
named. aws in your home directory. Home directory location varies but can be referred to using the
environment variables %UserProfile% in Windows and $HOME or ~ (tilde) in Unix-like systems.
For example, the following commands list the contents of the. aws folder:
Linux, macOS, or Unix
$ ls ~/.aws

In order to separate credentials from less sensitive options, region and output format are stored in a
separate file named config in the same folder

~/.aws/config
[default]
region=us-west-2
output=json

[profile user2]
region=us-east-1
output=text

The following settings are supported.
aws_access_key_id – AWS access key.
aws_secret_access_key – AWS secret key.
aws_session_token – AWS session token. A session token is only required if you are using
temporary security credentials.
region – AWS region.
output – output format (json, text, or table)


AWS CLI NAMED PROFILE
You can configure additional, named profiles by using the --profile option.
The AWS CLI supports named profiles stored in the config and credentials files. You can configure
additional profiles by using aws configure with the --profile option or by adding entries to the
config and credentials files.

Precedence for AWS CLI
The above configuration values have the following precedence:
• Command line options
• Environment variables
• Configuration file
AWS CLI SUB COMANDS
1. LIST------> List common configuration sources
2. GET-------> get the value of single configure value
3. SET ------> set the value of single configure var to see the list of configuration data
:~/.aws$ aws configure list ------> It will give output default aws profile
:~/.aws$ aws configure list --profile testuser


#####################################################################################################################################################################################################



VPC (Virtual private cloud)


Virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically
isolated from other virtual networks in the AWS cloud. You can launch your AWS resources, such
as Amazon EC2 instances, into your VPC. You can configure your VPC; you can select its IP
address range, create subnets, and configure route tables, network gateways, and security setting.


Subnet is a range of IP addresses in your VPC. You can launch AWS resources into a subnet that
you select. Use a public subnet for resources that must be connected to the Internet, and a private
subnet for resources that won't be connected to the Internet.


Private Subnet: A private subnet sets that route to a NAT instance. Private subnet instances only
need a private IP and internet traffic is routed through the NAT in the public subnet. You could also
have no route to 0.0.0.0/0 to make it a truly private subnet with no internet access in or out.


Public Subnet: A public subnet routes 0.0.0.0/0 through an Internet Gateway (igw). Instances in a
public subnet require public IPs to talk to the internet.


Network Address Translation (NAT) gateway is used to enable instances in a private subnet to
connect to the Internet or other AWS services, but prevent the Internet from initiating a connection
with those instances.

An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component
that allows communication between instances in your VPC and the Internet. It therefore imposes no
availability risks or bandwidth constraints on your network traffic.An Internet gateway serves two purposes: to provide a target in your VPC route tables for Internet-
routable traffic, and to perform network address translation (NAT) for instances that have been
assigned public IPv4 addresses. An Internet gateway supports IPv4 and IPv6 traffic.


A Route Table contains a set of rules, called routes that are used to determine where network
traffic is directed.
Each subnet in your VPC must be associated with a route table; the table controls the routing for the
subnet. A subnet can only be associated with one route table at a time, but you can associate
multiple subnets with the same route table.


Creating VPC:
❖ Go to VPC Dashboard from AWS main Dashboard as shown below, and click
on Start VPC Wizard
You will a VPC configuration page, select VPC with a single Public subnet
and click on select

Specify the IPV4 CIDR block range for subnet, provide a name for vpc and
click on create vpc.

❖ Your vpc is successfully created and is available to attach it to instances.

It takes few minutes come to available state. Once it is available select the vpc
and click on actions to attach it to the instance.

In the summary tab of vpc you can see the complete details of vpc.

Highly available VPC spans over multiple zones. Even if one zone goes down our services spanned
over the other zone will be still serving the user traffic. If you see from above diagram we have
web, DB and backend services in two zones. While creating Ec2 instance we can decide now on
which subnet our instance to create. So, for example we will create web01 in one subnet (located in
zone 1a) and web02(located in zone 1b) in other subnet. So, if zone 1a goes down we still have
web02 serving user traffic from zone 1b.
We are going to create HA VPC manually and not with the wizard.

❖ Creating VPC: Go to VPC from AWS main Dashboard.
❖ Click on your VPCs on left side of navigation pane and Click on Create VPC.
❖ Create CIDR block /16 and private IP range of your choice as shown below
❖ Go to Subnets and Click on create Subnets.
❖ Create first public subnet from same VPC range with /24 CIDR block.
❖ Select the Availability Zone as us-east-2a.
❖ Create second public subnet from same VPC range with /24 CIDR block.
❖ Select the Availability Zone as us-east-2b
❖ Create First PRIVATE subnet from same VPC range with /24 CIDR block.
❖ Select the Availability Zone as us-east-2a.
❖ Create Second PRIVATE subnet from same VPC range with /24 CIDR block.
❖ Select the Availability Zone as us-east-2b.
❖ Verify all your subnet settings.
❖ Go to Internet Gateway to Create Internet Gateway to map to Public subnet.
❖ Provide the name for Internet Gateway. Click on Create
❖ Attach IGW to your VPC:
❖ Create NAT Gateway to Map to your Private subnets.
❖ You also need an Elastic IP (EIP) to assign to your NAT Gateway.
❖ Go to NAT Gateway navigation pane and click on create NAT Gateway.
❖ Select any of the PUBLIC subnet that we created earlier, Create New EIP and
assign to Gateway.
❖ Once done click on create NAT Gateway
❖ NAT Gateway is successfully created as shown below.
❖ Go to Route Table to create route tables for subnets.
❖ Click on create Route Table and Give a name, Click on create.
❖ We need two route tables, one for Public subnet & one for Private Subnet.
❖ Add Route Table rule on HAPubRT to route to IGW that we created and save.
❖ Associate HAPubRT to Public subnets and save.
❖ Add Route Table rule on HAPrivRT to route to NAT GW and save.
❖ Associate HAPrivRT to PRIVATE subnets and save.
❖ Go to your Subnets and verify every subnet.
You can attach this VPC to EC2 instances with public subnet and private subnet.
If you want to connect to the private subnet instance first you need to connect to the public subnet
instance.

########################################################################################################################################################################################################



NACL (Network access control list)

Its act as a virtual firewall at a subnet level.

- VPC automatically gets a default NACL.
- Subnets are associated with NACLs. Subnets can only belong to a single NACL.
- Each NACl contain a set of rules that can allow or deny traffic into (inbound) and out of (outbound) subnets.
- You can allow or deny traffic, you could block a  single IP address (you cant do this with security groups).

#######################################################################################################################################################################################################


NAT (Network address translation)

Network address translation is a method of re-mapping one IP address space into another.
If you have a private network and you need to help gain outbound access to the internet you would need to use a NAt gateway to remap the Private IPs.
If you have 2 networks which have conflicting network address you can use a NAt to make the address more aggreeable.

#######################################################################################################################################################################################################

ELB (Elastic load balancer)
 
Distributes incoming application traffic across multiple targets, such as amazon EC2 instances, Containers, IP addresses, and Lambda functions.

Load balancers can be physical hardware or virtual software that accepts  incoming traffic, and then distributes the traffic to multiple targets. Can balance the load via different rules. These rules vary based on types of Loadbalancer.

- Application loadbalancer (ALB-HTTP/HTTPS)
- Network loadbalancer	(NLB-TCP/UDP)
- Classic loadbalancer	(CLB-Legacy)

OSI layers

Application
Presentation
Session
Transport
Network
Datalink
Physical

Sticky Sessions: Is a advamce load balancing method that allows you to bind a user's session to a specific EC2 instance.
Ensure all requests from that session are sent to the same instance.

#########################################################################################################################################################################################################


XFF (X-Forwarded-for) Header.

If you need the IPV4 address of a user, check the X-forwarded-For header.
The XFF header is a command method for identifying the originating IP address oa a client connecting to a web server through an HTTP proxy or a load balancer.

#########################################################################################################################################################################################################



EFS (Elastic File system)

Scalable. elastic, cloud native NFS file system. Attach a single file-system to multiple EC2 instances. EFS is a file storage service for EC2 instances, storage capacity  grows (upon petabytes) and shrinks automatically based on data storage(elastic). Multiple EC2 instances in same VPC can mount a single EFS volume(Volume must be in same volume), EC2 instances install the NFSv4.1 client and can then mount the EFS volume. EFS is using network file system version 4 (NFSv4) protocol, EFS creates multiple mount targets in all your VPC subnets.

First we create EFS then we create instances.

By this it will act like NFS mount directory between multiple instances , a directory will be mounted among all the instances and then you can share all the data you want.

###########################################################################################################################################################################################################



 EBS (Elastic block storage)

Amazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with
Amazon EC2 instances in the AWS Cloud. Each Amazon EBS volume is automatically replicated
within its Availability Zone to protect you from component failure, offering high availability and
durability. Amazon EBS volumes offer the consistent and low-latency performance needed to run
your workloads. With Amazon EBS, you can scale your usage up or down within minutes, all while
paying a low price for only what you provision.
Some of the key features and benefits that EBS volumes have to offer:
• High performance volumes
• Availability
• Encryption capabilities
• Snapshot capabilities
• Access Management
• Elastic Volumes


Note: EBS volumes cannot be copied from one AWS region to another. To make a volume
available outside of the Availability Zone, you can create a snapshot and restore that snapshot to a
new volume anywhere in that region. You can copy snapshots to other regions and then restore
them to new volumes there, making it easier to leverage multiple AWS regions for geographical
expansion, data centre migration, and disaster recovery.
Amazon EBS Volume Types:
There are three different types of EBS volumes, each with their own sets of performance
characteristics and associated costs:
General purpose volumes (SSD): This volume provides base performance of 3 IOPS/GiB, with
the ability to burst to 3,000 IOPS for extended periods of time. Gp2 volumes are ideal for a broad
range of use cases such as boot volumes, small and medium-size databases, and development and
test environments. Gp2 volumes support up to 10,000 IOPS and 160 MB/s of throughput.
Provisioned IOPS volumes (SSD): With Provisioned IOPS SSD (io1) volumes, you can provision
a specific level of I/O performance. Io1 volumes support up to 20,000 IOPS and 320 MB/s of
throughput. This allows you to predictably scale to tens of thousands of IOPS per EC2 instance.
Magnetic volumes: Magnetic volumes are backed by magnetic drives and are suited for workloads
where data is accessed infrequently, and scenarios where low-cost storage for small volume sizes is
important. These volumes deliver approximately 100 IOPS on average, with burst capability of up
to hundreds of IOPS, and they can range in size from 1 GiB to 1 TiB.

Creating, attaching, formatting & mounting EBS Volume to an EC2 instance:
Before we start knowing how to create Volume, let us create an EC2 instance of CentOS 6.
To view and access your account’s EBS Volumes using AWS Management Console, simply select
the Volumes option from the EC2 dashboard’s navigation pane.
Click Volumes in the left pane, we can see Volume Management Dashboard. From that select the
Create Volume option. 

❖ Fill in the details as required in the Create Volume dialog box. Here I created a
sample 5-GB general purpose volume.
❖ After filling the configuration settings, select Create to complete the volume
creation process. The new volume will take a few minutes to be available for
use as shown in the below figure. Once the volume is created, we can now
attach this volume to your running instance.
❖ The created volume is available for use. We will tag the volume by a name which is used for
future identification.
❖ Finally attach the volume to ec2 instance. We can attach multiple volumes to a single
instance at a time, with each volume having a unique device name. Some of these device
names are reserved, for example, /dev/sda1 is reserved for the root device volume.
❖ To attach a volume, select the volume which is available for use from the Volume
Management dashboard. Then select the Actions tab and click on the Attach Volume option.

Type your instance ID in the Instance field and provide a suitable name in the Device field as
shown. Here, I provided the recommended device name of /dev/sdf to this volume. Click on Attach

once you given the details. The volume attachment process takes a few minutes to complete. You
are now ready to make the volume accessible from your instance.
Mounting Volume to the instance:
After the volume is attached to an instance, you can format it and use it like other block device.
Here I’m using the same EC2 instance (CentOS6) that we created earlier. To get started, login to the
running instance using SSH.
As it is a CentOS machine by default it will login to the centos user. So, run the following
command to login to the root user and run the command to list the partitions of your instance. You
should see a default /dev/xvda partition along with its partition table and an unformatted disk
partition with the name /dev/xvdf as shown in the following screenshot. The /dev/xvdf command is
the newly added EBS volume that need to be formatted.

Now create new partition with the available disk partition as shown below.
# fdisk /dev/xvdf
Use m to list out various options that can be used in fdisk.
Use p to list out the partition information first and
Use n to create a new partition.

❖ After creating the partition, we have to format it with a filesystem of your
choice. Here I have chosen ext4 filesystem.

❖ Once formatting is done, we can mount that partition to a directory. Create a
directory and mount it to a volume.

Create some directories and files in that directory which is mounted to the volume.
Backup & Restore
We will create a situation where we need to restore the lost data.
We will delete few files from the mount point after taking the backup and then restore the deleted
data.
Backup the EBS volume by taking its snapshot:
Amazon EBS provides the ability to save point-in-time snapshots of your volumes to Amazon S3.
Amazon EBS Snapshots are stored incrementally: only the blocks that have changed after your last
snapshot are saved, and you are billed only for the changed blocks. If you have a device with 100
GB of data but only 5 GB has changed after your last snapshot, a subsequent snapshot consumes
only 5 additional GB and you are billed only for the additional 5 GB of snapshot storage, even
though both the earlier and later snapshots appear complete.
When you delete a snapshot, you remove only the data not needed by any other snapshot. All active
snapshots contain all the information needed to restore the volume to the instant at which that
snapshot was taken. The time to restore changed data to the working volume is the same for all
snapshots.
Snapshots can be used to instantiate multiple new volumes, expand the size of a volume, or move
volumes across Availability Zones. When a new volume is created, you may choose to create it
based on an existing Amazon EBS snapshot. In that scenario, the new volume begins as an exact
replica of the snapshot.
The following are key features of Amazon EBS Snapshots:
• Immediate access to Amazon EBS volume data
• Resizing Amazon EBS volumes
• Sharing Amazon EBS Snapshots
• Copying Amazon EBS Snapshots across AWS regions
To create a snapshot from the AWS Management Console, go to the Volume Management
Dashboard, Select the Volume, Click on Actions and Click on Create Snapshot.

Note: It’s a good practice to stop your instance before taking a snapshot if you are taking a snapshot
of its root volume.
You will see Create Snapshot dialog box as shown in the following screenshot. Provide Suitable
Name and Description for your new Snapshot. Here there is no Encryption for your snapshot
because the Volume is not encrypted. Snapshots of encrypted volumes will be encrypted
automatically.

❖ After filling the required details Click on Create to complete the snapshot
process. The process will take few minutes to complete.

❖ After creating the snapshot, we will delete few files from the mount point.
Create Volume from Snapshot & Resize:
Now if we want to recover the lost data, we have to create a new volume from snapshot & replace
old volume with the new one. We will also increase the volume size.
Click Snapshots in left pane of EC2 Dashboard, Select the snapshot from which new volume has to
be created, Click on Actions and select Create volume, you will get a create Volume pop up..

❖ Fill the details to create new volume. Here the new volume size must be
greater than 5GB because we created this snapshot from 5GB volume which is
attached to the instance. And the Availability Zone should be same where the
instance is created. Once this is done Click on Create to complete the process..

❖ Tag the old volume with -old extension for identification and tag new volume
with other name
❖ Once the volume is created it is available for use.

Unmount & detach Old Volume:
To unmount and detach old volume from the instance first check the mount points in the created
partition and then unmount the mounted directory

❖ Once we are done with unmounting then detach the old volume from the
instance. Select the volume to be detached, click on actions and select detach
volume.

Attach and mount new volume.
Finally attach the new volume to the instance by selecting the new volume, Click on Actions and
select Attach Volume. We will get Attach Volume pop up dialog box as shown below:
Provide the instance ID device partition.

❖ After attaching the volume mount, it to the instance as we done before.

we can see that the data is restored.
But the mount point size is still around 5 GB even though we changed the EBS volume size to 9
GB.
This is because the rest of the volume is still not resized and we need to resize it.
Resizing the increased volume:
We have already mounted the new volume but to resize it we need to unmount, delete partition,
recreate it with new size, resize it and mount it


Resize the Volume: In order to resize the volume, we need to run the command
# resize2fs /dev/xvdf2
(If we run this command it gives an error to run another command because new volume is greater
than old volume)

Note: If the new volume is also of same size as old volume then no need of resizing it, just we have
to mount it after attaching to the instance.

####################################################################################################################################################################################################



   Encrypted root volume


When you are through the wizard launching an EC2 instance you can encrypt  the volume on creation.


If you want to encrypt an existing volume you will have to do the following:

- Take a snapshot of the unencrypted volume.
- Create a copy of that snapshot and select the encryption option.
- Create a new AMI from the encrypted snapshot.
- Launch a new EC2 instance from the created AMI.

###################################################################################################################################################################################################


Cloud Front

A CDN is a distributed network of servers which delivers web pages and content to users based on their geographical location, the origin of the webpage, and a content delivery server.

####################################################################################################################################################################################################



AWS Cloud Watch

Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications we run
on AWS. It provides a reliable, scalable, and flexible monitoring solution that we can start using
within minutes. We no longer need to set up, manage, and scale our own monitoring systems and
infrastructure. We can use CloudWatch to collect and track metrics, which are variables we can
measure for our resources and applications. CloudWatch alarms send notifications or automatically
make changes to the resources we are monitoring based on rules that you define.
Features & Benefits:
• Monitor Amazon EC2 - View metrics for CPU utilization, data transfer, and disk usage
activity from Amazon EC2 instances for no additional charge. For an additional charge,
CloudWatch provides Detailed Monitoring for EC2 instances with higher resolution and
metric aggregation. No additional software needs to be installed.
• Monitor Other AWS Resources - Monitor metrics on Amazon DynamoDB tables,
Amazon EBS volumes, Amazon RDS DB instances, Amazon Elastic MapReduce job flows,
Elastic Load Balancers, Amazon SQS queues, Amazon SNS topics, and more for no
additional charge. No additional software needs to be installed.
• Monitor Custom Metrics - Submit Custom Metrics generated by your own applications via
a simple API request and have them monitored by Amazon CloudWatch.
• Monitor and Store Logs - You can use CloudWatch Logs to monitor and troubleshoot your
systems and applications using your existing system, application, and custom log files. You
can send your existing system, application, and custom log files to CloudWatch Logs and
monitor these logs in near real-time. This can help you better understand and operate your
systems and applications, and you can store your logs using highly durable, low-cost storage
for later access.
• Set Alarms - Set alarms on any of your metrics to send you notifications or take other
automated actions. For example, when a specific Amazon EC2 metric crosses your alarm threshold, you can use Auto Scaling to dynamically add or remove EC2 instances or send
you a notification.
View Graphs and Statistics - Amazon Cloudwatch Dashboards enable you to create re-
usable graphs of AWS resources and custom metrics so you can quickly monitor operational
status and identify issues at a glance.
Monitor and React to Resource Changes - CloudWatch Events provides a stream of
events describing changes to your AWS resources. You can easily build workflows that
automatically take actions you define, such as invoking an AWS Lambda function, when an
event of interest occurs.
Monitoring AWS EC2 instance CPU Utilization:
In order to monitor CPU Utilization we need an ec2 instance where we will cause high cpu load and
test.
I have created a centos 6 ec2 instance with tag Name: monittest and login to the instance.
For testing purpose Install stress tool, which can create a high CPU usage.

We can check lots of metrics by selecting the instance monitoring tab. All those graphs for CPU,
Disk & network usage collected by Cloudwatch monitoring tool by default. You can see data from
last hour to last two weeks.

You can see some AWS services available here. Click on Ec2

Click on CloudWatch service from AWS main dashboard.
Go to Metrics

There are some metrics available in EC2. Select Per-Instance metrics.
You can see the instance which we created along with the metrics attached to it.
Select the instance monittest with CPU utilization
As you can see above for instance monittest its showing cpu utilization graph for last 1 hour.
We can customize the graph display.
Click custom and select relative or absolute time to see graph details as per your wish.

Setting up Alarm.
Click on Alarms and select Create Alarm.

Search with instance ID to find all the metrics related to our instance.
Put a check mark on CPU Utilization against monittest.

Click on next. You have to give some details related to threshold and provide some name and
description to alarm.
In the actions section you have to specify the email id to which the notification to be received. Click
Create Alarm, once done.

You will get an email from AWS for verifying email address, once its verified you will start
receiving email alert whenever the instance cpu load crosses beyond 70 %.
Alarm has been created successfully for monittest instance.

To test that we can use stress utility and cause high cpu on our instance
Login from another tab and run top command to see it real time from the system.
Top command shows the current load average, running, sleeping and stopped processes of the
system.
Observe the load average, if it is above 80 then all the processes are stressed.
After few minutes you will receive an email in your inbox which shows Alarm and graph in red
colour.
It also gives an indication on alarm dashboard that the CPU utilization >=70 for 5 minutes.

Email from AWS

Monitoring AWS Billing:
We can also monitor our account’s estimated costs and usage by setting up an alarm.
Go to My Billing Dashboard ==> P

Go to CloudWatch service, Click on Billing and select create alarm.


Create Alarm:
Enter the threshold, if your AWS account charges exceeds than the threshold value you will receive
an email which you will provide in the send notification.
Click on create alarm.
View Alarm: your alarm has been successfully created for billing section of AWS.

#########################################################################################################################################################################################################

				ROUTE53



Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web
service. It is designed to give developers and businesses an extremely reliable and cost effective
way to route end users to Internet applications by translating names like www.example.com into the
numeric IP addresses like 192.0.2.1 that computers use to connect to each other. Amazon Route 53
is fully compliant with IPv6 as well.
Amazon Route 53 effectively connects user requests to infrastructure running in AWS such as
Amazon EC2 instances, Elastic Load Balancing load balancers, or Amazon S3 buckets and can also
be used to route users to infrastructure outside of AWS. You can use Amazon Route 53 to configure
DNS health checks to route traffic to healthy endpoints or to independently monitor the health of
your application and its endpoints.
STEPS TO CREATE HOSTED ZONE:
1) Buy a website from any DNS providers like GODADDY, etc..
2) Click on route53 service in AWS console
3) Click on create Hosted Zone
4) Enter your Domain name in the box mentioned like below
5) Click on create button to create hosted zone
6) After click on create button automatically some NS(Name Server) and SOA(Start Of
Authority) records are created like as shown as below
7) Copy the NS entities and paste it on your domain manager(GODADDY)
8) Now you create two EC2 instances in two different regions . And deploy an application
to each instance
9) Now back on to route53 console and create record set as shown as below
•
•
In name box write any name to access your webserver
In type box there are several options like A,CNAME etc..,we have to
select one of the option based on our requirement
✓ A------ipv4 address of the webserver
✓ CNAME------ to typing the CNAME we can give directly
domain name of the webserver instead of Alias name
✓ Mostly these two type of options are preferred
• In the value box we have to type ip address or dns name of webserver
• In routing policy we have select any one of the option based on your
requirement
✓ Simple: it is a default routing policy single alias name with
single webserver.
✓ Weighted: Based on the weight we have to mention in the
weight box how many no.of requests to go particular instances
in the different regions.
For weighted we have to specify same name to different record
set in the name box
1 st record set:............................refer document regarding this....


######################################################################################################################################################################################################



S3cmd

s3md is a command line utility used for creating s3 buckets, uploading, retrieving and managing
data to Amazon s3 storage.
Install s3cmd Package
s3cmd is available in default rpm repositories for CentOS, RHEL and Ubuntu systems, you can
install it using simply executing following commands on your system.
On Ubuntu/Debian:
$ sudo apt-get install s3cmd
Install Latest s3cmd from Source: -
If you are not getting latest version of s3cmd using above package managers, You can install last
s3cmd version on your system using source code.
Visit this url (http://ufpr.dl.sourceforge.net/project/s3tools/s3cmd/)
or
Use below command to download latest version of s3cmd.
$ wget http://ufpr.dl.sourceforge.net/project/s3tools/s3cmd/1.6.1/s3cmd-1.6.1.tar.gz
$ tar xzf s3cmd-1.6.1.tar.gz
Now install it using below command with source files.
$ cd s3cmd-1.6.1
$ sudo python setup.py install
Configure s3cmd Environment: -
In order to configure s3cmd we would require Access key and Secret key of your S3 Amazon
account.
After getting key files, use below command to configure s3cmd.
NOTE: - Before executing below command create a user with full permissions of s3 access in your
aws account.
$ s3cmd – configure
After press, the above command please enter the key file information like given below.
Enter new values or accept defaults in brackets with Enter.
Refer to user manual for detailed description of all options.
Access key and Secret key are your identifiers for Amazon S3
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Encryption password is used to protect your files from reading
by unauthorized persons while in transfer to S3
Encryption password: xxxxxxxxxx
Path to GPG program [/usr/bin/gpg]:
When using secure HTTPS protocol all communication with Amazon S3
servers is protected from 3rd party eavesdropping. This method is
slower than plain HTTP and can't be used if you're behind a proxy
Use HTTPS protocol [No]: Yes
New settings:
Access Key: xxxxxxxxxxxxxxxxxxxxxx
Secret Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
Encryption password: xxxxxxxxxx
Path to GPG program: /usr/bin/gpg
Use HTTPS protocol: True
HTTP Proxy server name:
HTTP Proxy server port: 0
Test access with supplied credentials? [Y/n] Y
Please wait, attempting to list all buckets...
Success. Your access key and secret key worked fine :-)
Now verifying that encryption works...
Success. Encryption and decryption worked fine :-)
Save settings? [y/N] y
Configuration saved to '/root/.s3cfg'
Now setup is ready to create s3 buckets, uploading, retrieving and managing data to Amazon s3
storage.
Uses of s3cmd Command Line: -
Once configuration is successfully completed. Now find below command details to how to manage
s3 buckets using commands.
1. List All S3 Bucket
Use following command to list all s3 buckets in your aws account.
$ s3cmd ls
2. Creating New Bucket
To create a new bucket in Amazon s3 use below command. It will create bucket named
venudevops (create your bucket name as your wish) in aws account.
$ s3cmd mb s3://venudevops
Bucket 's3://venudevops/' created
3. Uploading file in Bucket
Below command will upload file devops.txt to s3 bucket using s3cmd command.
$ s3cmd put devops.txt s3://venudevops/
devops.txt -> s3://venudevops/devops.txt
190216 of 190216
100% in
0s
[1 of 1]
1668.35 kB/s
done
4. Uploading Directory in Bucket
If we need to upload entire directory use -r to upload it recursively like below.
$ s3cmd put -r backup s3://venudevops/
backup/devops1.txt -> s3://venudevops/backup/devops1.txt
9984 of 9984
100% in
0s
18.78 kB/s
done
backup/devops2.txt -> s3://venudevops/backup/devops2.txt
0 of 0
0% in
0s
0.00 B/s
[1 of 2]
[2 of 2]
done
Make sure you are not adding trailing slash in upload directory named backup (eg: backup/), else it
will upload only content of backup directory only.
$ s3cmd put -r backup/ s3://venudevops/
backup/devops1.txt -> s3://venudevops/devops1.txt
9984 of 9984
100% in
0s
21.78 kB/s
done
backup/devops2.txt -> s3://venudevops/devops2.txt
0 of 0
0% in
0s
0.00 B/s
[1 of 2]
[2 of 2]
done
5. List Data of S3 Bucket
List the objects of s3 bucket using ls switch with s3cmd.
$ s3cmd ls s3://venudevops/
DIR
2017-06-03 10:58
190216
s3://venudevops/backup/
s3://venudevops/file.txt
6. Download Files from Bucket
Sometimes if we need to download files from s3 bucket, Use following commands to download it.
$ s3cmd get s3://venudevops/devops.txt
s3://venudevops/devops.txt -> ./devops.txt
4 of 4
100% in
0s
10.84 B/s
[1 of 1]
done
7. Remove Data of S3 Bucket
To remove files are folder from s3 bucket use following commands.
Removing file from s3 bucket
$ s3cmd del s3://venudevops/devops.txt
File s3://venudevops/devops.txt deleted
Removing directory from s3 bucket
$ s3cmd del s3://venudevops/backup
File s3://venudevops/backup deleted
8. Remove S3 Bucket
If we don’t need s3 bucket any more, we can simply delete it using following command. Before
removing bucket make sure its empty.
$ s3cmd rb s3://venudevops
ERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not
empty

####################################################################################################################################################################################################






